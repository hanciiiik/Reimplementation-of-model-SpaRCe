{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install reservoirpy torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFH5T8fYczxW",
        "outputId": "d3654e69-d765-49bd-b955-ba9c0a1e9f7a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: reservoirpy in /usr/local/lib/python3.12/dist-packages (0.4.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.12/dist-packages (from reservoirpy) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from reservoirpy) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from reservoirpy) (1.16.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from reservoirpy.nodes import Reservoir\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import trange\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiTltUkWc4PH",
        "outputId": "886b67a8-f358-4740-f356-8bc2754a4b12"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute ReservoirPy states\n",
        "def compute_reservoir_states(reservoir, X_images):\n",
        "    \"\"\"\n",
        "    X_images: numpy array shape [N_samples, 28, 28]\n",
        "    Produce for each sample a flattened vector of states: [N_units * T]\n",
        "    Return: states numpy array shape [N_samples, N_units * T]\n",
        "    \"\"\"\n",
        "    n_samples = X_images.shape[0]\n",
        "    T, in_dim = X_images.shape[1], X_images.shape[2]\n",
        "    # collect states per sample\n",
        "    states_out = []\n",
        "    for i in range(n_samples):\n",
        "        seq = X_images[i]  # shape [T, in_dim]\n",
        "        # reservoir.run expects shape (T, in_dim)\n",
        "        out = reservoir.run(seq)  # returns shape [T, n_units] (states per timestep)\n",
        "        # flatten timesteps and units into single vector\n",
        "        flat = out.reshape(-1)  # size n_units * T\n",
        "        states_out.append(flat)\n",
        "    return np.vstack(states_out).astype(np.float32)  # shape [n_samples, n_units*T]\n"
      ],
      "metadata": {
        "id": "8RuoQg5wdDLi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST format as sequences\n",
        "def load_mnist_seq(train=True):\n",
        "    # Download raw MNIST images as [N, 28, 28] floats in [0,1]\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    train_set = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "    test_set  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    X_tr = train_set.data.numpy().astype(np.float32) / 255.0  # [60000,28,28]\n",
        "    Y_tr = np.eye(10)[train_set.targets.numpy()]             # one-hot [60000,10]\n",
        "    X_te = test_set.data.numpy().astype(np.float32) / 255.0   # [10000,28,28]\n",
        "    Y_te = np.eye(10)[test_set.targets.numpy()]\n",
        "    return X_tr, Y_tr.astype(np.float32), X_te, Y_te.astype(np.float32)"
      ],
      "metadata": {
        "id": "ZkKveSIkdWjl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "RluNvtgOcarH"
      },
      "outputs": [],
      "source": [
        "# SpaRCe model\n",
        "class SpaRCeModel(nn.Module):\n",
        "    def __init__(self, in_features, n_classes, theta_i_init=None, theta_g=None):\n",
        "        \"\"\"\n",
        "        in_features: size [N_units * T] - flattened reservoir states\n",
        "        n_classes: number of output classes\n",
        "        theta_i_init: numpy array shape (in_features,) initial theta_i\n",
        "        theta_g: numpy array shape (in_features,) fixed global threshold\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # linear readout\n",
        "        self.readout = nn.Linear(in_features, n_classes, bias=False)\n",
        "\n",
        "        # local threshold theta_i trainable per neuron\n",
        "        if theta_i_init is None:\n",
        "            theta_i_init = np.random.randn(in_features).astype(np.float32) / (in_features**0.5)\n",
        "        self.theta_i = nn.Parameter(torch.from_numpy(theta_i_init).float())  # shape [in_features]\n",
        "\n",
        "        # fixed global threshold theta_g (non-trainable)\n",
        "        if theta_g is None:\n",
        "            theta_g = np.zeros((in_features,), dtype=np.float32)\n",
        "        self.register_buffer(\"theta_g\", torch.from_numpy(theta_g).float())  # buffer, not parameter\n",
        "\n",
        "    def forward(self, state):\n",
        "        \"\"\"\n",
        "        state: tensor [batch, in_features] (raw reservoir states)\n",
        "        returns logits [batch, n_classes] and state_sparse [batch, in_features]\n",
        "        Activation: state_sparse = sign(state) * relu(abs(state) - theta_g - theta_i)\n",
        "        \"\"\"\n",
        "        # broadcast thresholds\n",
        "        # formulas 4 and 5 from article\n",
        "        th = (self.theta_g + self.theta_i).unsqueeze(0)  # [1, in_features]\n",
        "        s_abs = torch.abs(state)\n",
        "        s_thresh = F.relu(s_abs - th)  # [batch, in_features]\n",
        "        state_sparse = torch.sign(state) * s_thresh\n",
        "        logits = self.readout(state_sparse)\n",
        "        return logits, state_sparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training of ESN or SpaRCe\n",
        "def train_models(States_tr, Y_tr, States_te, Y_te, States_val=None,\n",
        "                 MODEL=1,            # 1 = SpaRCe, 2 = Standard ESN\n",
        "                 Pns_list=[70],      # percentiles (only for SpaRCe)\n",
        "                 alpha_list=[1e-3],  # learning rates (for Standard ESN can be list)\n",
        "                 batch_size=128,\n",
        "                 N_episodes=2000,\n",
        "                 N_check=20,\n",
        "                 device=device):\n",
        "    \"\"\"\n",
        "    States_*: numpy arrays of shape [N_samples, in_features]\n",
        "    Y_*: numpy arrays (one-hot vectors) of shape [N_samples, n_classes]\n",
        "    Returns: Results_tr, Results_te, Results_val arrays\n",
        "    \"\"\"\n",
        "    States_tr = States_tr.astype(np.float32)\n",
        "    States_te = States_te.astype(np.float32)\n",
        "    if States_val is not None:\n",
        "        States_val = States_val.astype(np.float32)\n",
        "\n",
        "    N_train = States_tr.shape[0]\n",
        "    N_test  = States_te.shape[0]\n",
        "    N_class = Y_tr.shape[1]\n",
        "    in_features = States_tr.shape[1]\n",
        "\n",
        "    # splits for evaluation: mirroring original code logic\n",
        "    train_divide = 100\n",
        "    test_divide  = 50\n",
        "    val_divide   = 50\n",
        "\n",
        "    N_train_d = int(np.floor(N_train / train_divide))\n",
        "    N_test_d  = int(np.floor(N_test / test_divide))\n",
        "    if States_val is not None:\n",
        "        N_val = States_val.shape[0]\n",
        "        N_val_d = int(np.floor(N_val / val_divide))\n",
        "    else:\n",
        "        N_val = 0\n",
        "        N_val_d = 0\n",
        "\n",
        "    # prepare PyTorch datasets for training (will sample random indices)\n",
        "    train_dataset = TensorDataset(torch.from_numpy(States_tr), torch.from_numpy(Y_tr))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
        "\n",
        "    # prepare results containers\n",
        "    if MODEL == 1:\n",
        "        N_copies = len(Pns_list)\n",
        "        Results_tr = np.zeros((N_copies, N_check, 3), dtype=np.float32)  # [error, accuracy, coding]\n",
        "        Results_te = np.zeros((N_copies, N_check, 3), dtype=np.float32)\n",
        "        Results_val= np.zeros((N_copies, N_check, 3), dtype=np.float32)\n",
        "    else:\n",
        "        N_copies = len(alpha_list)\n",
        "        Results_tr = np.zeros((N_copies, N_check, 2), dtype=np.float32)  # [error, accuracy]\n",
        "        Results_te = np.zeros((N_copies, N_check, 2), dtype=np.float32)\n",
        "        Results_val= np.zeros((N_copies, N_check, 2), dtype=np.float32)\n",
        "\n",
        "    # loss function: sigmoid cross entropy (BCEWithLogits for multi-label)\n",
        "    criterion = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "\n",
        "    # function to evaluate a model on a dataset by splitting into smaller parts called 'divisions'\n",
        "    def evaluate_model(model, States, Y, division):\n",
        "        model.eval()\n",
        "        total_loss = 0.0\n",
        "        total_correct = 0\n",
        "        total_count = 0\n",
        "        coding_sum = 0.0\n",
        "        n_parts = division\n",
        "        part_size = int(np.floor(States.shape[0] / n_parts)) if n_parts>0 else States.shape[0]\n",
        "        # iterate parts\n",
        "        for part in range(n_parts):\n",
        "            start = part * part_size\n",
        "            end = (part + 1) * part_size\n",
        "            Xp = torch.from_numpy(States[start:end]).to(device)\n",
        "            Yp = torch.from_numpy(Y[start:end]).to(device)\n",
        "            with torch.no_grad():\n",
        "                logits, state_sparse = model(Xp)\n",
        "                loss = criterion(logits, Yp)\n",
        "                total_loss += loss.item() * (end - start)\n",
        "                preds = torch.argmax(torch.sigmoid(logits), dim=1)\n",
        "                labels = torch.argmax(Yp, dim=1)\n",
        "                total_correct += (preds == labels).sum().item()\n",
        "                total_count += (end - start)\n",
        "                # coding level (fraction of non-zero activations)\n",
        "                if hasattr(model, \"theta_i\"):\n",
        "                    coding_sum += (state_sparse != 0).float().sum().item()\n",
        "        # processing the rest\n",
        "        rem = States.shape[0] - n_parts * part_size\n",
        "        if rem > 0:\n",
        "            start = n_parts * part_size\n",
        "            Xp = torch.from_numpy(States[start:]).to(device)\n",
        "            Yp = torch.from_numpy(Y[start:]).to(device)\n",
        "            with torch.no_grad():\n",
        "                logits, state_sparse = model(Xp)\n",
        "                loss = criterion(logits, Yp)\n",
        "                total_loss += loss.item() * rem\n",
        "                preds = torch.argmax(torch.sigmoid(logits), dim=1)\n",
        "                labels = torch.argmax(Yp, dim=1)\n",
        "                total_correct += (preds == labels).sum().item()\n",
        "                total_count += rem\n",
        "                if hasattr(model, \"theta_i\"):\n",
        "                    coding_sum += (state_sparse != 0).float().sum().item()\n",
        "        avg_loss = total_loss / total_count\n",
        "        acc = total_correct / total_count\n",
        "        coding = None\n",
        "        if hasattr(model, \"theta_i\"):\n",
        "            coding = coding_sum / (total_count * (in_features / States.shape[1]) )  # simplified: fraction of non-zero elements per sample\n",
        "            # compute coding fraction per-sample\n",
        "            coding = coding_sum / (total_count * in_features)\n",
        "        model.train()\n",
        "        return avg_loss, acc, coding\n",
        "\n",
        "    # MAIN LOOP: train separate models per copy\n",
        "    for copy_idx in range(N_copies):\n",
        "        print(f\"\\n=== Training copy {copy_idx+1}/{N_copies} ===\")\n",
        "        # build model and optimizers depending on MODEL\n",
        "        # if model is SpaRCe\n",
        "        if MODEL == 1:\n",
        "            # compute theta_g_start for this copy (percentile across training states per feature)\n",
        "            P = Pns_list[copy_idx]\n",
        "            # states_tr of shape [N_train, in_features] -> compute percentile per feature\n",
        "            theta_g = np.percentile(np.abs(States_tr), P, axis=0).astype(np.float32)  # shape [in_features,]\n",
        "            theta_i_init = (np.random.randn(in_features).astype(np.float32) / max(1.0, in_features**0.5))\n",
        "            model = SpaRCeModel(in_features, N_class, theta_i_init=theta_i_init, theta_g=theta_g).to(device)\n",
        "            # optimizers: one for readout (output layer), one for theta_i (trainable neural thresholds)\n",
        "            alpha = 1e-3 if len(alpha_list)==0 else alpha_list[0]\n",
        "            opt_readout = torch.optim.Adam(model.readout.parameters(), lr=alpha)\n",
        "            opt_theta   = torch.optim.Adam([model.theta_i], lr=alpha/10.0) # gradient descent rule\n",
        "            # step both optimizers each batch\n",
        "        else:\n",
        "            # Standard ESN: no trainable theta_i, only linear readout\n",
        "            model = nn.Linear(in_features, N_class, bias=False).to(device)\n",
        "            # wrap into a small wrapper to reuse evaluation code expecting model(X)->(logits,state_sparse)\n",
        "            class StdWrapper(nn.Module):\n",
        "                def __init__(self, linear):\n",
        "                    super().__init__()\n",
        "                    self.linear = linear\n",
        "                def forward(self, state):\n",
        "                    logits = self.linear(state)\n",
        "                    return logits, None\n",
        "            model = StdWrapper(model)\n",
        "            # pick learning rate for this copy\n",
        "            alpha = alpha_list[copy_idx]\n",
        "            opt_readout = torch.optim.Adam(model.parameters(), lr=alpha)\n",
        "            opt_theta = None\n",
        "\n",
        "        # training loop with checkpoints\n",
        "        check_interval = max(1, int(np.round(N_episodes / N_check)))\n",
        "        train_iter = trange(N_episodes, desc=f\"Copy {copy_idx+1}\")\n",
        "        for n in train_iter:\n",
        "            # one mini-batch update (sample random batch)\n",
        "            batch_idx = np.random.randint(0, N_train, size=(batch_size,))\n",
        "            batch_x = torch.from_numpy(States_tr[batch_idx]).to(device)\n",
        "            batch_y = torch.from_numpy(Y_tr[batch_idx]).to(device)\n",
        "            # forward\n",
        "            logits, state_sparse = model(batch_x)\n",
        "            loss = criterion(logits, batch_y)\n",
        "            # backward and update\n",
        "            opt_readout.zero_grad()\n",
        "            if opt_theta is not None:\n",
        "                opt_theta.zero_grad()\n",
        "            loss.backward()\n",
        "            opt_readout.step()\n",
        "            if opt_theta is not None:\n",
        "                opt_theta.step()\n",
        "\n",
        "            # checkpoint evaluation\n",
        "            if n % check_interval == 0:\n",
        "                idx = int(n / check_interval)\n",
        "                # evaluate on test/train/val as authors do\n",
        "                # test\n",
        "                if MODEL == 2:\n",
        "                    loss_te, acc_te, _ = evaluate_model(model, States_te, Y_te, test_divide)\n",
        "                    loss_tr, acc_tr, _ = evaluate_model(model, States_tr, Y_tr, train_divide)\n",
        "                    if States_val is not None:\n",
        "                        loss_val, acc_val, _ = evaluate_model(model, States_val, Y_val, val_divide)\n",
        "                    else:\n",
        "                        loss_val, acc_val = 0.0, 0.0\n",
        "                    Results_te[copy_idx, idx, 0] = loss_te\n",
        "                    Results_te[copy_idx, idx, 1] = acc_te\n",
        "                    Results_tr[copy_idx, idx, 0] = loss_tr\n",
        "                    Results_tr[copy_idx, idx, 1] = acc_tr\n",
        "                    if States_val is not None:\n",
        "                        Results_val[copy_idx, idx, 0] = loss_val\n",
        "                        Results_val[copy_idx, idx, 1] = acc_val\n",
        "                    print(f\"[ESN] Iter {n} copy {copy_idx} TEST acc={acc_te:.4f} loss={loss_te:.4f}\")\n",
        "                else:\n",
        "                    loss_te, acc_te, coding_te = evaluate_model(model, States_te, Y_te, test_divide)\n",
        "                    loss_tr, acc_tr, coding_tr = evaluate_model(model, States_tr, Y_tr, train_divide)\n",
        "                    if States_val is not None:\n",
        "                        loss_val, acc_val, coding_val = evaluate_model(model, States_val, Y_val, val_divide)\n",
        "                    else:\n",
        "                        loss_val, acc_val, coding_val = 0.0, 0.0, 0.0\n",
        "                    Results_te[copy_idx, idx, 0] = loss_te\n",
        "                    Results_te[copy_idx, idx, 1] = acc_te\n",
        "                    Results_te[copy_idx, idx, 2] = coding_te if coding_te is not None else 0.0\n",
        "                    Results_tr[copy_idx, idx, 0] = loss_tr\n",
        "                    Results_tr[copy_idx, idx, 1] = acc_tr\n",
        "                    Results_tr[copy_idx, idx, 2] = coding_tr if coding_tr is not None else 0.0\n",
        "                    if States_val is not None:\n",
        "                        Results_val[copy_idx, idx, 0] = loss_val\n",
        "                        Results_val[copy_idx, idx, 1] = acc_val\n",
        "                        Results_val[copy_idx, idx, 2] = coding_val if coding_val is not None else 0.0\n",
        "                    print(f\"[SpaRCe] Iter {n} copy {copy_idx} TEST acc={acc_te:.4f} loss={loss_te:.4f} coding={coding_te:.4f}\")\n",
        "\n",
        "    return Results_tr, Results_te, Results_val\n"
      ],
      "metadata": {
        "id": "CUPqj8wIdemc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# running the whole program\n",
        "if __name__ == \"__main__\":\n",
        "    # Load MNIST and convert to sequences - columns\n",
        "    X_tr, Y_tr, X_te, Y_te = load_mnist_seq()\n",
        "    # reshape to [N, T, in_dim] where T=28 (timesteps), in_dim=28 (features per timestep)\n",
        "    X_tr_seq = X_tr.reshape(-1, 28, 28)\n",
        "    X_te_seq = X_te.reshape(-1, 28, 28)\n",
        "\n",
        "    # build an ESN with reservoirpy\n",
        "    # hyperparams (example)\n",
        "    N_units = 300         # reservoir size\n",
        "    sr = 0.97             # spectral radius\n",
        "    input_scaling = 1.0   # gamma\n",
        "    density = 0.05        # internal sparsity\n",
        "\n",
        "    # create ReservoirPy Reservoir node\n",
        "    res = Reservoir(units=N_units, sr=sr, input_scaling=input_scaling, input_connectivity=density, activation=np.tanh)\n",
        "\n",
        "    # compute reservoir states\n",
        "    print(\"Computing reservoir states for training set...\")\n",
        "    S_tr = compute_reservoir_states(res, X_tr_seq)  # shape [N_train, N_units*T]\n",
        "    print(\"Computing reservoir states for test set...\")\n",
        "    S_te = compute_reservoir_states(res, X_te_seq)\n",
        "\n",
        "    # example: train SpaRCe (MODEL=1) with two Pns (70% and 90%)\n",
        "    Pns_list = [70]           # list of percentiles to test\n",
        "    alpha_list = [1e-3]       # learning rate used for readout (SpaRCe uses alpha and alpha/10 for theta)\n",
        "    Results_tr, Results_te, Results_val = train_models(S_tr, Y_tr, S_te, Y_te,\n",
        "                                                       States_val=None,\n",
        "                                                       MODEL=1,\n",
        "                                                       Pns_list=Pns_list,\n",
        "                                                       alpha_list=alpha_list,\n",
        "                                                       batch_size=128,\n",
        "                                                       N_episodes=1000,\n",
        "                                                       N_check=10,\n",
        "                                                       device=device)\n",
        "\n",
        "    print(\"Training finished.\")\n",
        "    print(\"Results (test):\", Results_te)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm7XBlcTdzWW",
        "outputId": "864cdb44-8849-40d1-f6d6-4814776e1c38"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing reservoir states for training set...\n",
            "Computing reservoir states for test set...\n",
            "\n",
            "=== Training copy 1/1 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:   1%|          | 7/1000 [00:06<11:30,  1.44it/s]  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 0 copy 0 TEST acc=0.2042 loss=0.6087 coding=0.3020\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  11%|█         | 107/1000 [00:13<03:58,  3.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 100 copy 0 TEST acc=0.8768 loss=0.1123 coding=0.3111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  21%|██        | 208/1000 [00:21<02:47,  4.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 200 copy 0 TEST acc=0.9054 loss=0.0820 coding=0.3182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  31%|███       | 311/1000 [00:29<02:35,  4.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 300 copy 0 TEST acc=0.9184 loss=0.0689 coding=0.3232\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  41%|████      | 406/1000 [00:37<03:14,  3.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 400 copy 0 TEST acc=0.9263 loss=0.0619 coding=0.3269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  51%|█████     | 508/1000 [00:45<01:53,  4.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 500 copy 0 TEST acc=0.9321 loss=0.0570 coding=0.3299\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  61%|██████    | 606/1000 [00:53<01:22,  4.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 600 copy 0 TEST acc=0.9381 loss=0.0528 coding=0.3325\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  71%|███████   | 712/1000 [01:01<01:00,  4.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 700 copy 0 TEST acc=0.9397 loss=0.0499 coding=0.3345\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  81%|████████  | 808/1000 [01:08<00:42,  4.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 800 copy 0 TEST acc=0.9431 loss=0.0470 coding=0.3362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1:  91%|█████████ | 911/1000 [01:16<00:17,  5.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SpaRCe] Iter 900 copy 0 TEST acc=0.9462 loss=0.0453 coding=0.3377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copy 1: 100%|██████████| 1000/1000 [01:17<00:00, 12.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training finished.\n",
            "Results (test): [[[0.6086691  0.2042     0.3020059 ]\n",
            "  [0.11225568 0.8768     0.31110823]\n",
            "  [0.08195557 0.9054     0.3181737 ]\n",
            "  [0.06889166 0.9184     0.3232063 ]\n",
            "  [0.06186916 0.9263     0.3269354 ]\n",
            "  [0.05697567 0.9321     0.32989866]\n",
            "  [0.05282497 0.9381     0.33246282]\n",
            "  [0.0498951  0.9397     0.33448672]\n",
            "  [0.04698935 0.9431     0.3361568 ]\n",
            "  [0.045253   0.9462     0.33770385]]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}